\section{Linear system}
\exo{1}{0}
\nosolution

\exo{2}{1}
\begin{solution}
  \begin{itemize}
    \item If $x \in \Ker A$, then $Ax = 0$
      which also means that $RAX = R0 = 0$ so $X \in \Ker(RA)$.
    \item If $y \in \Ima(AR)$, then there is a $x$
      such that $ARx = y$
      which also means that there is a $x'$ such that
      $Ax' = y$ since we can just take $x' = Rx$.
      So $y \in \Ima A$.
  \end{itemize}
\end{solution}

\exo{3}{2}
\begin{solution}
  Let's suppose by contradiction that there are 2 ways
  to do this
  \begin{align}
    \label{eq:2.3.1}
    \mathbf{x} & = \sum_{i=1}^k \alpha_i \mathbf{a}_i\\
    \label{eq:2.3.2}
               & = \sum_{i=1}^k \beta_i \mathbf{a}_i
  \end{align}
  which $\beta_i \neq \alpha_i$ for at least one $i \in \{0, \ldots, k\}$.
  $\eqref{eq:2.3.2}-\eqref{eq:2.3.1}$ gives
  \[
    0 = \sum_{i=1}^k (\beta_i-\alpha_i) \mathbf{a}_i.
  \]
  by hypothesis, there is a $i$ such that $\beta_i-\alpha_i \neq 0$
  which is a contradiction with the linear independence of the $\mathbf{a}_i$.
\end{solution}

\exo{4}{4}
\begin{solution}
  Let's suppose by conradiction that there are 2
  basis $a_1, \ldots, a_n$ and $b_1, \ldots, b_m$ with $n < m$.

  Let's just remember that $a_1, \ldots, a_n$ is spanning
  and $b_1, \ldots, b_m$ are linearly independent.

  We have therefore that
  \[ b_1 = \alpha_1 a_1 + \ldots + \alpha_n a_n\]
  with at least one $\alpha_i \neq 0$ since $b_1 \neq 0$.
  Wlog, let's say that $\alpha_1 \neq 0$.
  Therefore
  \[ a_1 = \frac{1}{\alpha_1}b_1 + \frac{\alpha_2}{\alpha_1} a_2 + \ldots + \frac{\alpha_n}{\alpha_1} a_n\]
  so $(b_1, a_2, \ldots, a_n)$ is spanning.

  We now have
  \[ b_2 = \beta_1 b_1 + \beta_2 a_2 + \ldots + \beta_n a_n\]
  if $\beta_2 = \cdots = \beta_n = 0$, $b_1, b_2$ are not linearly independent.
  Wlog, let's say that $\beta_2 \neq 0$,
  $(b_1,b_2,a_3,\ldots,a_n)$ is therefore spanning.

  Continuing this reasoning, $(b_1,\ldots,b_n)$ is spanning.
  $b_{n+1}$ is therefore a linear combination of $b_1, \ldots, b_n$ which is a contradiction.
\end{solution}

\exo{5}{2}
\begin{solution}
  Since $\mathbb{C}^{m \times n}$ is a vector space and if $X,Y \in \mathbb{C}^{m \times n}$, $\trace(Y^*X) \in \mathbb{C}$ which is a field,
  $\trace(Y^*X)$ could be the scalar product $(X,Y)$.
  Let's check.
  Since
  \begin{align*}
    \trace(X^*X)
    & = \sum_{i = 1}^m \sum_{j = 1}^n \overline{x_{ij}}x_{ij}\\
    & = \sum_{i = 1}^m \sum_{j = 1}^n |x_{ij}|^2,
  \end{align*}
  we clearly have $\trace(X^*X) \geq 0$ for all $X \in \mathbb{C}^{m \times n}$ and
  $\trace(X^*X) \Longleftrightarrow X = 0$.
  We also have
  \begin{align*}
    \trace(Z^*(\alpha X + \beta Y))
    & = \trace(\alpha Z^*X + \beta Z^*Y)\\
    & = \alpha\trace(Z^*X) + \beta\trace(Z^*Y)\\
    \trace(Y^*X)
    & = \trace((X^*Y)^*)\\
    & = \overline{\trace(X^*Y)}.
  \end{align*}
  Since it is a scalar product, we can apply the Schwarz inequality which gives
  \begin{align*}
    |\trace(Y^*X)|
    & \leq \sqrt{\trace(X^*X)} \sqrt{\trace(Y^*Y)}\\
    & = \|X\|_F \|X\|_F.
  \end{align*}
\end{solution}

\exo{6}{1}
\begin{solution}
  We have
  \begin{align*}
    A^*A
    & = R^*Q^*QR\\
    & = R^*IR\\
    & = R^*R.
  \end{align*}

  Note that since $A$ is squared
  (since otherwise it wouldn't be defined for it to be positive definite)
  and that $A$ is full rank (if not, it has an zero eigenvalue and cannot be positive definite).
  Therefore $R$ is an upper triangular square matrix with no zero element in this case.
\end{solution}

\exo{7}{2}
\begin{solution}
  \begin{description}
    \item[$\Rightarrow$]
      It must be true $\forall x \in \mathcal{X}$ and $\forall y \in \mathcal{Y}$ so in particular,
      it should be true for every element of the base.
      So we need to have $(x_i,y_j) = 0$ for all $i,j$ so in other words
      \[
        \begin{pmatrix}
          y_1 & y_2 & \cdots & y_n
        \end{pmatrix}^*
        \begin{pmatrix}
          x_1 & x_2 & \cdots & x_n
        \end{pmatrix}
        = 0.
      \]
    \item[$\Leftarrow$]
      Let $\sum a_ix_i$ be an element of $\mathcal{X}$ and $\sum b_iy_i$ be an element of $\mathcal{Y}$.
      We need
      \begin{align*}
        (\sum_i a_ix_i, \sum_i b_iy_i)
        & = \sum_i a_i (x_i, \sum_j b_jy_j)\\
        & = \sum_i a_i \overline{(\sum_j b_jy_j, x_i)}\\
        & = \sum_i \sum_j a_i\overline{b_j} \overline{(y_j, x_i)}\\
        & = \sum_i \sum_j a_i\overline{b_j} (x_i, y_j)\\
        & = \sum_i \sum_j a_i \overline{b_j} 0\\
        & = 0.
      \end{align*}
  \end{description}
\end{solution}

\exo{8}{3}
\begin{solution}
  \begin{enumerate}
    \item
      \begin{align*}
        (S^\perp)^\perp
        & = \{x | (x,y) = 0, \forall y \in \mathcal{S}^\perp\}\\
        & = \{x | (x,y) = 0, \forall y \in \{z | (z,w) = 0, \forall w \in \mathcal{S}\}\}.
      \end{align*}
      In other word, $x$ should be perpendicular to every vector which is perpendicular to all vectors of $\mathcal{S}$.
      This is clearly true for $x \in \mathcal{S}$.
      If $x \notin \mathcal{S}$, since it is perpendicular to every vector of $\mathcal{S}^\perp$, $x \notin \mathcal{S}^\perp$,
      this contradicts the lemma~2.8.
    \item
      \begin{align*}
        (\mathcal{S}_1 + \mathcal{S}_2)^\perp
        & = \{x | (x,y) = 0, \forall y \in (\mathcal{S}_1+\mathcal{S}_2)\}\\
        & = \{x | (x,\alpha y_1 + \beta y_2) = 0, \forall \alpha,\beta, \forall y_1 \in \mathcal{S}_1, \forall y_2 \in \mathcal{S}_2\}\\
        & = \{x | ((x, y_1) = 0, \forall y_1 \in \mathcal{S}_1) \land ((x, y_2) = 0, \forall y_2 \in \mathcal{S}_2)\}\\
        & = \{x | (x, y_1) = 0, \forall y_1 \in \mathcal{S}_1\} \cap \{x | (x, y_2) = 0, \forall y_2 \in \mathcal{S}_2\}\\
        & = \mathcal{S}_1^\perp \cap \mathcal{S}_2^\perp.
      \end{align*}
    \item
      $(\mathcal{S}^\perp)^\perp$ actually ensures us that
      2 subspaces are equal iff their orthogonal complements are equal.
      If they are equal, it is obvious that their complement is equal
      and if their complement is equal, the complement of their complement is equal so their are equal.

      We can therefore prove that the complements are equals
      \begin{align*}
        ((\mathcal{S}_1 \cap \mathcal{S}_2)^\perp)^\perp
        & = (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp\\
        \mathcal{S}_1 \cap \mathcal{S}_2
        & = (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp
      \end{align*}
      Using the previous property, we have
      \begin{align*}
        (\mathcal{S}_1^\perp + \mathcal{S}_2^\perp)^\perp
        & = (\mathcal{S}_1^\perp)^\perp \cap (\mathcal{S}_2^\perp)^\perp\\
        & = \mathcal{S}_1 \cap \mathcal{S}_2.
      \end{align*}
  \end{enumerate}
\end{solution}

\exo{9}{0}
\nosolution

\exo{10}{0}
\nosolution

\exo{11}{0}
\nosolution

\exo{12}{0}
\nosolution

\exo{13}{0}
\nosolution

\exo{14}{0}
\nosolution

\exo{15}{0}
\nosolution

\exo{16}{0}
\nosolution

\exo{17}{0}
\nosolution

\exo{18}{0}
\nosolution

\exo{19}{0}
\nosolution

\exo{20}{0}
\nosolution
